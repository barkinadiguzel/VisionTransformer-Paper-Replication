# ğŸ–¼ï¸ Vision Transformer (ViT) From Scratch â€” *Replicating â€œAn Image is Worth 16x16 Wordsâ€*

Reimplementation of the **Vision Transformer (ViT) architecture** proposed in  
ğŸ“„ [Dosovitskiy et al., 2020 â€” *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929)

This project reproduces the ViT model entirely **from scratch using PyTorch**.  
Every component â€” from **patch embedding** and **class token**, **positional encoding**, **multi-head self-attention**, **feed-forward layers**, to the **encoder stack** â€” follows the original paper and equations, with formulas visually mapped in [`images/summary.png`](images/summary.png).

---

## Model Flow Summary

![ViT Summary](images/summary.png)

- This summary visual matches the ViT architecture with its core formulas. Each stepâ€”from patch embedding to the final class tokenâ€”is linked to the corresponding section in the original paper.

---

## ğŸ§© Project Structure
```bash

VisionTransformer-Paper-Replicating/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ input_embedding/
â”‚ â”‚ â”œâ”€â”€ patch_embedding.py         â†’ Patch â†’ Linear Embedding (makale: Section 3.2)
â”‚ â”‚ â””â”€â”€ positional_encoding.py     â†’ Sinusoidal or Learnable (makale: Section 3.2)
â”‚ â”‚
â”‚ â”œâ”€â”€ attention/
â”‚ â”‚ â”œâ”€â”€ scaled_dot_product.py      â†’ softmax(QKáµ€ / âˆšdâ‚–)V (makale: Section 3.2.1)
â”‚ â”‚ â””â”€â”€ multi_head_attention.py    â†’ Concat(headâ‚,â€¦,headâ‚•)Wâ‚€ (makale: Section 3.2.2)
â”‚ â”‚
â”‚ â”œâ”€â”€ feed_forward/
â”‚ â”‚ â””â”€â”€ positionwise_ffn.py        â†’ FFN(x)=max(0,xWâ‚+bâ‚)Wâ‚‚+bâ‚‚ (makale: Section 3.3)
â”‚ â”‚
â”‚ â”œâ”€â”€ encoder/
â”‚ â”‚ â”œâ”€â”€ encoder_layer.py           â†’ Attention + FFN + Residual + LayerNorm (makale: Section 3.1)
â”‚ â”‚ â””â”€â”€ encoder_stack.py           â†’ N-layer encoder stack (makale: Section 3.1)
â”‚ â”‚
â”‚ â”œâ”€â”€ vit_model/
â”‚ â”‚ â””â”€â”€ vit_assembly.py            â†’ Patch Embedding + Encoder + Class token â†’ final ViT (makale: Section 3)
â”‚ â”‚
â”‚
â”œâ”€â”€ images/
â”‚ â””â”€â”€ summary.png
â”‚
â””â”€â”€ requirements.txt

```
---
## ğŸ”— Feedback

For feedback or questions, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)


